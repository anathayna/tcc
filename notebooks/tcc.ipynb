{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anathayna/tcc/blob/main/notebooks/tcc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAWb9q-onvzZ"
      },
      "source": [
        "# <font color=\"orange\">**TCC: Identificação do discurso de ódio em memes**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1EYDJR-oXhf"
      },
      "source": [
        "O Google Colab é um serviço do Jupyter Notebook hospedado que oferece acesso gratuito a recursos de computação, incluindo GPUs e TPUs. Ele é adequado principalmente para aprendizado de máquina, ciência de dados e educação.\n",
        "\n",
        "E este notebook documenta todo o desenvolvimento do trabalho de conclusão de curso que tem o objetivo de identificar o discurso de ódio em memes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nna8A1RMotNH"
      },
      "source": [
        "# <font color=\"orange\">**Sumário**</font>\n",
        "\n",
        "1.   Instalando as bibliotecas e dependências\n",
        "2.   Adicionando o banco de dados\n",
        "3.   Extração de características\n",
        "\n",
        "Rascunho:\n",
        "* Processamento das imagens\n",
        "* Processamento dos textos\n",
        "* Fine-tuning pre-treinamento\n",
        "* Validação (fine-tuned)\n",
        "* Gerando previsões\n",
        "* Calculando probabilidades\n",
        "* Classificação dos memes\n",
        "* Avaliação (métricas)\n",
        "* Conclusão\n",
        "\n",
        "Preparar os dados: carregar train/val do dataset Hateful Memes.\n",
        "\n",
        "Extrair embeddings com CLIP (imagem e texto).\n",
        "\n",
        "Definir protótipos ou palavras-chave de referência para ativação fuzzy (e.g., “raça”, “religião”, imagens com símbolos ofensivos).\n",
        "\n",
        "Calcular graus fuzzy separadamente para cada modalidade.\n",
        "\n",
        "Criar regras fuzzy (talvez usando scikit-fuzzy ou outra biblioteca).\n",
        "\n",
        "Inferir grau final de hatefulness e treinar ou calibrar thresholds com a base rotulada.\n",
        "\n",
        "Avaliar com AUROC, F1, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtRzcnVrpYYJ"
      },
      "source": [
        "## <font color=\"orange\">1. Instalando as bibliotecas e dependências</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qOgE5wsm-f7"
      },
      "source": [
        "Primeiro, configure o ambiente de execução do Google Colab para o GPU T4, que é voltado para aceleração de inferência de modelos de aprendizado profundo.\n",
        "\n",
        "Outro detalhe que o Google Colab já vem com CUDA pré-instalado e drivers de GPU.\n",
        "\n",
        "O **CUDA** (Compute Unified Device Architecture) é uma plataforma de computação paralela desenvolvida pela NVIDIA para acelerar processamentos usando GPUs (Graphics Processing Units). Ele permite que programas utilizem o poder de processamento massivamente paralelo das GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY-k0GoYnT2b",
        "outputId": "223384e5-55ca-4679-9020-e9e31473f021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvcc: command not found\n",
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version  # Verifica se o CUDA está disponível\n",
        "!nvidia-smi  # Mostra detalhes da GPU (como versão suportada)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjTFtoctsAvu"
      },
      "source": [
        "Próximo passo é conferir se o PyTorch 1.7.1 (ou uma versão superior) e o torchvision estão instalados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaMMONp0xpki",
        "outputId": "db3d3367-97e7-47db-d057-c76737286df1"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchvision'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchvision'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch\n",
        "from pkg_resources import packaging\n",
        "\n",
        "print(\"Versão do Torch:\", torch.__version__)\n",
        "print(\"Versão do Torchvision:\", torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPfs5cs31s0z"
      },
      "source": [
        "Após conferir as instalações iniciais, serão instaladas as seguintes dependências: o pacote *ftfy* (para normalização de texto), a biblioteca *regex* (processamento de strings) e *tqdm* (monitoramento de progresso)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI4bvrCIxS73",
        "outputId": "df6adc01-24f7-4d1d-bc9c-c280416ff860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "%pip install ftfy regex tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr-hkhrU3FlD"
      },
      "source": [
        "Essas instalações são pré-requisito para a execução do modelo CLIP (Contrastive Language–Image Pretraining) da OpenAI, que será a metodologia central deste trabalho. Em seguida, será feito a instalação do CLIP por meio de sua implementação oficial disponível no GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BT_YydI3Glo",
        "outputId": "b638e353-ad13-4891-d3e2-2fb7cbca72f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-rou6mwig\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-rou6mwig\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.23.0+cu126)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ62yqqW3MOg"
      },
      "source": [
        "O CLIP fornece os seguintes modelos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW5re7Fq3dfL",
        "outputId": "3aa361d8-29fe-4ba4-a911-120e2ac980e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14',\n",
              " 'ViT-L/14@336px']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLdn76296ji2"
      },
      "source": [
        "*   **ViT-B/32**: mais rápido e leve\n",
        "*   **ViT-L/14**: tem maior precisão e alcança melhores resultados, mas exigem mais GPU\n",
        "*   **ViT-B/16**: equilíbrio entre velocidade e acurácia\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgqhurXhAQVd"
      },
      "source": [
        "Tais modelos permitem analisar e extrair propriedades relevantes para seu uso, como o **ViT-B/32** que é o mais utilizado. Ele possui resolução de entrada de 224x224 pixels, contexto textual máximo de 77 tokens e vocabulário de cerca de 49 mil tokens.\n",
        "\n",
        "A análise inclui o cálculo do total de parâmetros (aproximadamente 151 milhões), métrica que quantifica a complexidade e capacidade de aprendizado da rede.\n",
        "\n",
        "Isso evidencia como avaliar escalabilidade e limitações operacionais de modelos multimodais, cuja operação demanda alto poder computacional e impõe restrições ao tamanho de textos e à resolução de imagens processáveis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJjClg9szqjI",
        "outputId": "8075cceb-c467-4f1c-f8b6-25671c3e2ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total de parâmetros do modelo: 151,277,313\n",
            "Resolução das imagens: 224\n",
            "Comprimento de contexto: 77\n",
            "Tamanho do vocabulário: 49408\n"
          ]
        }
      ],
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "model.cuda().eval()\n",
        "\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Total de parâmetros do modelo:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Resolução das imagens:\", input_resolution) # ex: 224x224\n",
        "print(\"Comprimento de contexto:\", context_length) # número máximo de tokens\n",
        "print(\"Tamanho do vocabulário:\", vocab_size) # quantidade de palavras/tokens únicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Pc7Zmg-2FW"
      },
      "source": [
        "## <font color=\"orange\">2. Adicionando o banco de dados</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucWcEkHe_Aob"
      },
      "source": [
        "Serão utilizados dois bancos de dados para a identificação do discurso de ódio em memes.\n",
        "\n",
        "O primeiro é o **Hateful Memes** disponibilizado pela Meta no *Challenge Hateful Memes*, um banco de dados com mais de **10 mil** imagens de memes em inglês, que contêm conteúdo ofensivo relacionado a gênero, raça, religião, orientação sexual, classe social e outros tópicos.\n",
        "\n",
        "E o conjunto de dados é composto pelas seguintes porcentagens:\n",
        "\n",
        "- 40% de memes de ódio multimodal (multimodal hate)\n",
        "- 10% de memes de ódio unimodal (unimodal hate)\n",
        "- 20% de memes com confusão de texto benigna (benign text confounder)\n",
        "- 20% de memes com confusão de imagem benigna (benign image confounder)\n",
        "- 10% de memes não odiosos aleatórios (random non-hateful)\n",
        "\n",
        "Os **confundidores benignos** consistem em imagens ou textos alternativos para cada meme odioso, que alteram sua classificação para não odioso. Esses confundidores foram adicionados para garantir que o conjunto de dados seja apropriado para testar a verdadeira capacidade multimodal de um modelo, levando em conta que, no mundo real, o modelo se depararia com exemplos diversos e inéditos.\n",
        "\n",
        "![](https://drivendata-public-assets.s3.amazonaws.com/memes-overview.png)\n",
        "\n",
        "**Figura 1:** Exemplo de meme utilizado na competição  \n",
        "Fonte: DRIVENDATA (2020)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXokoaMp7C2z"
      },
      "source": [
        "**TO-DO:**\n",
        "- MMHS150k dataset (multimodal hate speech)\n",
        "- Memotion dataset (sentiment classification of memes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFKfKRTuN-eW"
      },
      "source": [
        "Baixe o banco de dados do **Hateful Memes** pela plataforma *Kaggle* no seguinte endereço: https://www.kaggle.com/datasets/williamberrios/hateful-memes\n",
        "\n",
        "Originalmente, esse conjunto de dados estava disponível no site oficial do desafio (https://hatefulmemeschallenge.com/#download). Entretanto, no momento da elaboração desta pesquisa, o domínio encontrava-se indisponível."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "m5iCOp2AFfoo"
      },
      "outputs": [],
      "source": [
        "#@markdown Defina o caminho para o arquivo **.zip** do banco de dados do *Hateful Memes*.\n",
        "#@markdown **exemplo:** `\"/content/drive/MyDrive/hateful_memes.zip\"`\n",
        "\n",
        "PATH_TO_ZIP_FILE = '/content/drive/MyDrive/hateful_memes.zip' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Defina o diretório base para extrair o banco de dados.\n",
        "#@markdown **exemplo:** `\"/content\"`\n",
        "\n",
        "HOME = '/content' #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frXoE0qDAVzR"
      },
      "source": [
        "Após realizar o download do banco de dados e armazená-lo no Google Drive, é necessário integrá-lo ao ambiente do Google Colab para extração das características. Para isso, utilizamos a biblioteca google.colab.drive, que permite montar o Google Drive como um sistema de arquivos virtual no ambiente de execução.\n",
        "\n",
        "Ao executar o código, o Colab solicitará uma autorização para acessar sua conta do Google Drive. Uma vez concedida a permissão, o Drive será vinculado ao diretório `/content/drive/`, possibilitando a leitura dos arquivos armazenados. Essa abordagem garante acesso contínuo aos dados durante o processamento, sem necessidade de uploads manuais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmKCLjjvJki-",
        "outputId": "650ace5e-a2f7-4aaa-cadc-4037f8696a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IyozyXsDe3d"
      },
      "source": [
        "Agora, preparamos o ambiente para incorporar a pasta \"model\", que armazenará o banco de dados e outros arquivos essenciais, garantindo que o projeto possa acessá-los corretamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx7h6xlgIpfG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(HOME)\n",
        "os.getcwd()\n",
        "os.environ['PYTHONPATH'] += \":/content/model/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07FrRc0CFjku"
      },
      "source": [
        "Em seguida, o código realiza a extração automática do banco de dados Hateful Memes (armazenado no formato `.zip`) para o diretório `/content/model/`, disponibilizando os arquivos para as próximas etapas de processamento e análise no notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9k26lVWLME5u"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_path = PATH_TO_ZIP_FILE\n",
        "extract_path = '/content/model/'\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcOQ3AIHPD_F",
        "outputId": "33622377-3c3a-40b3-b7af-9065669c8854"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total de imagens: 12140\n"
          ]
        }
      ],
      "source": [
        "img_dir = '/content/model/hateful_memes/img'\n",
        "\n",
        "image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']\n",
        "\n",
        "image_count = 0\n",
        "for filename in os.listdir(img_dir):\n",
        "    if any(filename.lower().endswith(ext) for ext in image_extensions):\n",
        "        image_count += 1\n",
        "\n",
        "print(f\"total de imagens: {image_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzWVadF6HlFN"
      },
      "source": [
        "## <font color=\"orange\">3. Extração de características</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCrRLOuuLWxH"
      },
      "source": [
        "Extraia texto e imagem com os rótulos (0/1).\n",
        "pré-processamento de texto: tokenização, deteção de insultos, análise de sentimento\n",
        "\n",
        "crie variáveis fuzzy: por ex. InsultoText? com valores “baixo”, “médio”, “alto”; ImagemAgressiva? com níveis “nenhum”, “moderado”, “forte”.\n",
        "\n",
        "regras\n",
        "IF InsultoText é alto AND ImagemAgressiva é forte THEN GrauHate é alto\n",
        "IF InsultoText é baixo AND ImagemAgressiva é baixo THEN GrauHate é baixo\n",
        "\n",
        "fuzzificação, cálculo das regras, agregação e desfuzzificação para cada meme, gerando um grau contínuo de “ódio”\n",
        "\n",
        "Converta o grau fuzzy em decisão: defina um limiar (ex: >= 0.5 → “hateful”, caso contrário “não-hateful”)\n",
        "\n",
        "- load model\n",
        "- prepare inputs: image & text\n",
        "- calculate features: image & text encode = encoded features\n",
        "- clip.tokenize\n",
        "- model.encode_image\n",
        "- model.encode_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkA8xGiDXG7T",
        "outputId": "f82df58b-1d9e-4160-d1f9-f089cdebd705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines) (25.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n",
            "Label: 0\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas jsonlines\n",
        "\n",
        "import os\n",
        "import jsonlines\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class HatefulMemesDataset(Dataset):\n",
        "    def __init__(self, jsonl_path, img_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            jsonl_path (str): Caminho para o arquivo .jsonl (ex: \"train.jsonl\").\n",
        "            img_dir (str): Caminho para a pasta com as imagens (ex: \"img/\").\n",
        "            transform (callable, optional): Pré-processamento (ex: `preprocess` do CLIP).\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Lê o arquivo .jsonl e converte para DataFrame\n",
        "        with jsonlines.open(jsonl_path) as reader:\n",
        "            self.data = pd.DataFrame(reader)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data.iloc[idx][\"img\"]\n",
        "        # Correct the path by removing the 'img/' prefix from img_name\n",
        "        # if it exists, before joining with the base image directory.\n",
        "        if img_name.startswith(\"img/\"):\n",
        "             img_name = img_name[4:] # Remove 'img/'\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        label = self.data.iloc[idx][\"label\"]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label  # Retorna (imagem, rótulo)\n",
        "\n",
        "base_path = '/content/model/hateful_memes/'\n",
        "img_dir = os.path.join(base_path, \"img\")\n",
        "train_jsonl = os.path.join(base_path, \"train.jsonl\")\n",
        "\n",
        "import clip\n",
        "\n",
        "# Carrega o modelo CLIP e o pré-processador\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device)\n",
        "\n",
        "# Cria o dataset de treino\n",
        "train_dataset = HatefulMemesDataset(\n",
        "    jsonl_path=train_jsonl,\n",
        "    img_dir=img_dir,\n",
        "    transform=preprocess\n",
        ")\n",
        "\n",
        "# Exemplo: visualizar uma amostra\n",
        "image, label = train_dataset[0]\n",
        "print(\"Label:\", label)  # 0 ou 1\n",
        "\n",
        "# Conjuntos de desenvolvimento e teste\n",
        "dev_seen_dataset = HatefulMemesDataset(\n",
        "    jsonl_path=os.path.join(base_path, \"dev_seen.jsonl\"),\n",
        "    img_dir=img_dir,\n",
        "    transform=preprocess\n",
        ")\n",
        "\n",
        "test_unseen_dataset = HatefulMemesDataset(\n",
        "    jsonl_path=os.path.join(base_path, \"test_unseen.jsonl\"),\n",
        "    img_dir=img_dir,\n",
        "    transform=preprocess\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True  # Embaralha os dados para treino\n",
        ")\n",
        "\n",
        "dev_loader = DataLoader(\n",
        "    dev_seen_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False  # Não embaralha para avaliação\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5deec2f6"
      },
      "source": [
        "# Task\n",
        "Elaborar um plano de trabalho para a identificação de discurso de ódio em memes, incluindo a extração de características visuais e textuais, a incorporação de múltiplos datasets, o fine-tuning de um modelo pré-treinado (CLIP), a validação do modelo, a classificação dos memes, a análise dos resultados e a documentação do processo para um trabalho de conclusão de curso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5df17ba"
      },
      "source": [
        "## Extração de características\n",
        "\n",
        "### Subtask:\n",
        "Processar as imagens e textos utilizando o modelo CLIP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adcb8e02"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the data loaders to process images and texts using the CLIP model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97oh6jpt65Jq",
        "outputId": "e34c73aa-d1cc-405b-f661-9c42d69b25b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train image features shape: torch.Size([8500, 512])\n",
            "Train labels shape: torch.Size([8500])\n",
            "Dev image features shape: torch.Size([500, 512])\n",
            "Dev labels shape: torch.Size([500])\n",
            "Test unseen image features shape: torch.Size([2000, 512])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device) # Ensure model is on the correct device\n",
        "\n",
        "def get_features(dataloader, dataset, model, device):\n",
        "    all_image_features = []\n",
        "    all_text_features = []\n",
        "    all_labels = [] # Collect labels for train and dev sets\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            # Get the original text from the dataset using indices\n",
        "            # Assuming the dataloader provides indices or the order is preserved\n",
        "            # This part needs careful handling based on how the dataset and dataloader are implemented\n",
        "            # For simplicity here, we'll assume we can get the text based on the batch\n",
        "            # A more robust solution might involve modifying the dataset to return text along with image and label\n",
        "\n",
        "            # Placeholder for text extraction - this needs to be adapted based on dataset structure\n",
        "            # If the dataset returns indices, use them to get text from the dataframe\n",
        "            # If not, and shuffle is True (train_loader), the order is not guaranteed\n",
        "            # For now, we'll skip text encoding as the current dataset implementation doesn't provide text in __getitem__\n",
        "            # We need to modify the HatefulMemesDataset to return text as well.\n",
        "\n",
        "            # For now, let's just process images\n",
        "            image_features = model.encode_image(images)\n",
        "            all_image_features.append(image_features.cpu())\n",
        "            all_labels.append(labels) # Collect labels\n",
        "\n",
        "    return torch.cat(all_image_features), torch.cat(all_labels)\n",
        "\n",
        "# Note: The current HatefulMemesDataset implementation does not return text.\n",
        "# To process text with CLIP, the dataset needs to be modified to include text in its __getitem__ method.\n",
        "# For now, we will only extract image features.\n",
        "\n",
        "# Example usage (extracting image features):\n",
        "train_image_features, train_labels = get_features(train_loader, train_dataset, model, device)\n",
        "dev_image_features, dev_labels = get_features(dev_loader, dev_seen_dataset, model, device)\n",
        "\n",
        "# For test_unseen, there are no labels to collect\n",
        "def get_test_features(dataloader, dataset, model, device):\n",
        "    all_image_features = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, _ in dataloader: # Assuming dataloader still returns a second element (placeholder)\n",
        "            images = images.to(device)\n",
        "            image_features = model.encode_image(images)\n",
        "            all_image_features.append(image_features.cpu())\n",
        "\n",
        "    return torch.cat(all_image_features)\n",
        "\n",
        "# Create a DataLoader for test_unseen_dataset\n",
        "test_unseen_loader = DataLoader(\n",
        "    test_unseen_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_unseen_image_features = get_test_features(test_unseen_loader, test_unseen_dataset, model, device)\n",
        "\n",
        "\n",
        "print(\"Train image features shape:\", train_image_features.shape)\n",
        "print(\"Train labels shape:\", train_labels.shape)\n",
        "print(\"Dev image features shape:\", dev_image_features.shape)\n",
        "print(\"Dev labels shape:\", dev_labels.shape)\n",
        "print(\"Test unseen image features shape:\", test_unseen_image_features.shape)\n",
        "\n",
        "# To process text, the HatefulMemesDataset needs to be updated to return text.\n",
        "# Once the dataset is updated, the get_features function can be modified to encode text as well."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPF2cE/lEz72Kzlx0bGcG1+",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
